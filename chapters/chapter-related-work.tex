% !TEX root = ../thesis.tex
%
\chapter{Related Work}
\label{sec:related}
In this chapter we quickly want to revise other works touching the tasks of this thesis. In \treft{sec:related:detection} we discuss multiple successful object detection methods using \glspl{cnn} and in \treft{sec:related:retrieval} we list previous image retrieval systems also focusing on those employing \glspl{cnn}.

\section{Object detection}
\label{sec:related:detection}
The first work to approach object detection with a \gls{cnn} were \citet{girshick_rich_2014}, with their \textbf{\gls{rcnn}}. For each image they use Selective Search to generate a set of region proposals. Selective Search is a technique to identify object-like, but type independent, regions probing texture and color of the image at multiple scales. All these boxes are reshaped to a uniform size and forwarded through a normal classification network (in their case AlexNet \citep{krizhevsky_imagenet_2012}) to yield feature vectors from the \gls{fc} layers. Lastly a multi-class \gls{svm} classifies these proposals. Additionally the \gls{rcnn} is trailed by a linear regression model which uses the classified boxes to produce tighter box coordinates. Although the results proved that \glspl{cnn} can greatly improve detection results, the method is hard to train because the classifier network, the \gls{svm} and the regressor have to be trained independently. Also at test time classifying each region box independently is extremely slow.

A year later again \citet{girshick_fast_2015} released \textbf{Fast \gls{rcnn}} which improves computation performance and training complexity. Instead of computing features for each proposed region, Fast \gls{rcnn} computes the features for the whole image at once. Then the regions in the image are projected onto the feature map of the network, thus yielding the patch-wise features. Those patch-wise feature maps are pooled, forwarded through \gls{fc} layers and finally into a classifier and the box regressor. The \gls{svm} is replaced with a SoftMax and the linear regression model with a additional \gls{fc} layer. As such the three models from \gls{rcnn} become one unified network which is faster and easier to train.

In the same year \citet{ren_faster_2015} tackled the big bottleneck of this approach, which is the region proposal system, with their \textbf{Faster \gls{rcnn}}. The previously used Selective Search is slow and still independent of the model. Using also the features generated by the \gls{conv} layers, they add a new network branch, a \gls{rpn}, to generate box proposals. These are used for the same \gls{roi} pooling as in the Fast \gls{rcnn}. The \gls{rpn} is a \gls{fcn} that takes the feature map and at each positions tests for a predefined set of $k$ anchor boxes whether those could be objects. So for each window the \gls{rpn} outputs a score of \textit{objectness} and coordinates of the bounding box. Those are then the inputs for the Fast \gls{rcnn} method.

This year \citet{he_mask_2017} extended this approach with \textbf{Mask \gls{rcnn}} to pixel-wise segmentation of images. In the previous versions the regions from the original image were projected onto the feature maps by just down-scaling their shapes with the respective factor and discretizing them on the feature grid. This rounding introduces errors in the proposed bounding boxes and leads to misalignment. Instead they use bilinear interpolation inside each region to find more precise feature representations. This process is done by a new branch inside the Faster \gls{rcnn} and outputs binary pixel-wise masks for each \gls{roi}. The regions are classified as in the Faster \gls{rcnn} and as such they are able to return semantic image segmentations.

\citet{dai_r-fcn:_2016} incorporates the \gls{roi}-wise processing into the main network branch of their \textbf{R-FCN} through a series of \gls{conv} layers. The \gls{conv} layers encode scores at relative positions like top-left or bottom-center. Next the regions from the \gls{rpn} are divided into the same relative sub-regions and each sub-region is used to pool from the corresponding layer and then to vote for the whole \gls{roi}.

As our work also has to focus on time performance, it is related to other fast object detectors. \citet{redmon_you_2016} use a freshly designed network architecture \textbf{YOLO} to achieve object detection at real-time. The input image is divided into a regular grid. The last \gls{fc} layer from their network predicts for each cell of the grid the class probabilities and the coordinates and objectness confidence of two bounding boxes. Detections get selected by the multiplication of their conventional class probabilities and their individual confidence predictions. While the method is extremely fast it is limited by the shape of bounding boxes it will predict.
\clearpage
\section{Image retrieval}
\label{sec:related:retrieval}
Retrieving relevant images from large databases by searching for similarities to an exemplar image, is a long used idea. The first methods employed hand-crafted features like histograms, SIFT or \gls{hog} to compare different images in a low dimensional space. Problems in such approaches arise with semantically close groups of objects with wide visual differences.\\
One of the more classical but successful approaches is \textbf{OASIS} \citep{chechik_large_2010}. Here the authors solve a bilinear similarity function for whole images using sparse local pattern descriptors. While the approach is successful under its training settings it is heavily restricted in that it has to be trained on all needed image classes.

\citet{wu_online_2013} learns image similarity using stacked neural networks. The inputs for those are low image features and they learn representations of modalities of these features giving a optimized multi-model combination. With a bag of five features they achieve better results, retrieving similar images, than OASIS but also lack wider applicability through the complexity of training.

The step forward to just learning similarity on top of raw pixels is done by \citet{babenko_neural_2014}, as they show that using just the distance between  the outputs of \gls{fc} layers, it is possible to retrieve visually similar images. More sophisticated is \textbf{DeepRanking} \citep{wang_learning_2014} use the deep feature of an AlexNet, together with visual features extracted by convolutions, in a linear embedding as a image descriptor. With the distance between the embeddings as measure, they rank the similarity of images to a query image. The visual features and the linear embedding are learned by feeding the network triplets of query, positive, and negative images. They show that the learned features outperform hand-crafted ones.

Neural networks shown success in being used in reverse image search systems. In \citep{khosla_building_2015} they train different network architectures on images of shoes and use a distance measure between the descriptors from the trailing \gls{fc} layer to retrieve alike shoes to a query shoe. Similarly the image descriptions from a task-specifically fine-tuned network are used in \citep{chen_visual-based_2015} to select similar styles inside clothing catalogs. Using the idea of \citep{wang_learning_2014} \citet{bell_learning_2015} learn a embedding of image descriptors. They use a siamese network architecture to learn from tuples of similar or dissimilar pairs. With the learned embedding space they retrieve similar interior designs given a query product by picking close training samples in the embedding space.
