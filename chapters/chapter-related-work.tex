% !TEX root = ../thesis.tex
%
\chapter{Related Work}
\label{sec:related}
In this chapter we quickly want to revise other works touching the the tasks of this thesis. In \treft{sec:related:detection} we discuss multiple successful object detection methods using \glspl{cnn} and in \treft{sec:related:retrieval} we list previous image retrieval systems focusing on those employing \glspl{cnn}.

\section{Object detection}
\label{sec:related:detection}
The first work to approach object detection with \glspl{cnn} where \citet{girshick_rich_2014} with their \textbf{\gls{rcnn}}. For each image they use Selective Search to generate a set of region proposals. Selective Search is a technique to identify possible object-like but type independent regions probing texture and color on multiple scales. All these boxes are reshaped to a uniform size and forwarded through a normal classification network (in their case AlexNet) to yield feature vectors from the \gls{fc} layers for each box. Lastly a multi-class \gls{svm} classifies these proposols. Additionally the \gls{rcnn} is trailed by a linear regression model which uses the classified boxes to produce tighter box coordinates. Although the results proved that \glspl{cnn} can greatly improve detection results the method is rather hard to train as classifier network, \gls{svm} and regressor have to be trained independently and moreover classifying each region box independently is extremely slow.

A year later again \citet{girshick_fast_2015} released \textbf{Fast \gls{rcnn}} which improves computation performance and training complexity. Instead of computing features for each proposed region Fast \gls{rcnn} computes the features for the whole image once. Then the regions in the image are projected onto the feature map of the network thus yielding the patch-wise features. Then those are pooled and put through \gls{fc} layers and finally into a classifier and the box regressor. The \gls{svm} is replaced with a SoftMax and the linear regression model with a additional \gls{fc} layer. As such the three models from \gls{rcnn} become one which is are end-to-end trainable.

In the same year \citet{ren_faster_2015} tackled the last bottleneck of this approach which is the region proposal system with their \textbf{Faster \gls{rcnn}}. The previously used Selective Search is rather slow and still independent of the model. Using also the features generated by the \gls{conv} layers they add a new network branch, a \gls{rpn}, to generate boxes that are used for the same \gls{roi} pooling as in the Fast \gls{rcnn}. The \gls{rpn} is a \gls{fcn} that slides over the feature map and at each positions tests for a predefined set of $k$ anchor boxes whether those could be objects. So for each window the \gls{rpn} outputs a score of \textit{objectness} and coordinates of the bounding box. Those are then the inputs for the Fast \gls{rcnn} method.

Two year late \citet{he_mask_2017} extended this approach with \textbf{Mask \gls{rcnn}} to pixel-wise segmentation of images. In the previous versions the regions from the original image were projected onto the feature maps by just down-scaling their shapes with the respective factor and discretizing them on the feature grid. This rounding introduces errors in the proposed bounding boxes and leads to misalignment. Instead they use bilinear interpolation inside each region to find more precise feature representations. This process is done by a new network branch inside the Faster \gls{rcnn} and outputs binary pixel-wise masks for each \gls{roi}. The regions are classified as in the Faster \gls{rcnn} and generate therefore semantic image segmentations.

\citet{dai_r-fcn:_2016} incorporates the \gls{roi}-wise processing into the main network branch of their \textbf{R-FCN} through a series of \gls{conv} layers. The \gls{conv} layers encode scores at relative positions like top-left or bottom-center. Next the regions from the \gls{rpn} are divided into the same relative sub-regions and each sub-region is used to pool from the corresponding layer and then vote for the whole \gls{roi}.

As out work also has to focus on time performance it is related to other fast object detectors. \citet{redmon_you_2016} use a freshly designed network architecture \textbf{YOLO} to achieve object detection at real-time. The input image is divided into a regular grid. The last \gls{fc} layer from their network predicts for each cell of the grid the class probabilities and the coordinates and objectness confidence of two bounding boxes. Detections then get selected by the multiplication of their conventional class probabilities and their individual confidence predictions. While the method is extremely fast it is limited by the shape of bounding boxes it will predict.
\clearpage
\section{Image retrieval}
\label{sec:related:retrieval}
Retrieving relevant images from large databases by searching for similarities to an exemplar image is an long used idea. The first methods employed hand-crafted features like histograms, SIFT or \gls{hog} to compare different images in a low dimensional space. Problems in such approaches arise through semantically close groups with wide visual differences.\\
One of the more classical but successfull approaches is \textbf{OASIS} \citep{chechik_large_2010}. Here the authors solve a bilinear similarity function for whole images using sparse local pattern descriptors. While the approach is successful under its training settings it is heavily restricted in that it has to be trained on all needed image classes.

\citet{wu_online_2013} learns image similarity using stacked neural networks. The inputs for those are low image features and they learn representations of modalities of these features giving a optimized multi-model combination. With a bag of five features they achieve better results retrieving similar images than OASIS but also lack wider applicability through the complexity of training.

The step forward to just learning similarity on top of raw pixels is done by \citet{babenko_neural_2014} as they show that using just the distance between  the outputs of \gls{fc} layers it is possible to retrieve visually similar images. More sophisticated \citet{wang_learning_2014} use the deep feature of an AlexNet\citep{krizhevsky_imagenet_2012} together with visual features extracted by convolutions in a linear embedding as a image descriptor. With the distance between the embeddings as measure they rank the similarity of images to a query image. The visual features and the linear embedding is learned by feeding the network triplets of query, positive and negative images. They show that the learned features outperform hand-crafted ones.

Neural networks shown success in being used in reverse image search systems. In \citep{khosla_building_2015} they train different network architectures on images of shoes and use a distance measure between the descriptors from the trailing \gls{fc} layer to retrieve alike shoes to a query shoe. Similarly the image descriptions from a task-specifically fine-tuned network are used in \citep{chen_visual-based_2015} to select similar styles inside clothing catalogues. Using the idea of \citep{wang_learning_2014} \citet{bell_learning_2015} learn a embedding of image descriptors. They use a siamese network architecture to learn from tuples of similar or dissimilar pairs. With this embedding space they retrieve similar interior designs given a query product.
