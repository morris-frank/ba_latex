% !TEX root = ../thesis.tex
%
\chapter{Test results}
\label{sec:results}
Testing of the method is done on the \textsc{Pascal}-Part dataset \citep{Chen_2014_CVPR} which provides part-based segmentation annotations for the \textsc{Pascal}-VOC 2010 challenge dataset \citep{pascal-voc-2010}. For the 20 object classes it provides binary segmentation maps for parts of all objects.\\
For testing the detection method we train a model with different number of images. From \textsc{Pascal}-Part we pick sets of classes $c \subset C = \{\code{bird, person, train,}\dotsc\}$ and parts $p \subset P = \{\code{head, beak, lhand,}\dotsc\}$. We gather all images from \textsc{Pascal}-VOC 2010 which contain parts $p$ from the classes $c$. Then we extract all patches of these images around the selected parts. We call this set of image patches $I_{c,p}$, e.g. all images of beaks of birds. Then we pick a random subset $I_k$ of $k$ images from $I_{c,p}$ and train the network with this subset. We want to investigate the influence of $k$ on detection results. The remaining set of images $I_{c,p} \ I_k$ will be the test set for this single network.

Following in this chapter we first rationalize our reduced and unequal number of tests \tref{sec:results:repeat}. Next in \treft{sec:results:baseline} we quickly explain the baseline method that was used to compare our results. Finally in \treft{sec:results:results} we report our test results in detection success and time performance \tref{sec:results:time}.

\section{Repeated training and testing}
\label{sec:results:repeat}
Obviously we also have to consider the quality of the subset not only its size. As we know that a part of the generated and augmented samples is way to small or contains no significant structure to learn we have to assume that some image patches are not useful for training. Some patches are smaller than $10\times10$ pixels and others are nearly monochrome. To avoid the influence of bad images we repeat the training with one set size $k$ multiple times and calculate the mean results of the multiple runs.

Lets assume 30\% of all image patches are useless for training. For how many times do we have to repeat the training with $k$ images so that we can neglect the influence of these bad samples? To get a rough estimate we reduce this problem to a binary one. First we compute how probable it is that a training will fail for one $k$.\\
The probability $p_b$ that one image is bad is $0.3$. Lets further assume for simplicity that a training will still succeed sufficiently if up to $v = 40\%$ of the samples are bad. We get the probability that the training will succeed from the \gls{cdf} of the binomial distribution:
\begin{equation}
    p_s(k) = B_{k,p_b, v k} = \sum_{i=0}^{v k} \binom{k}{i}\cdot p_b^i \cdot (1 - p_b)^{k - i}
\end{equation}
For $k\in\{1, 10, 25, 50, 100\}$ we get $p_s = \{\num{0.7},\num{0.85},\num{0.902},\num{0.952},\num{0.988}\}$, respectively. These are the probabilities that we will not fail one specific training. The probability that a training fails is $p_f = 1 - p_s = \{\num{0.3},\num{0.15},\num{0.098},\num{0.048},\num{0.012}\}$. Now we want to look at the probability for an $n$ times repeated run of $k$ images that half or less of these rounds fail. Again this can be computed with the \gls{cdf}:
\begin{equation}
    p_{repeat}(n,p_f) = B_{n,p_f, \sfrac{n}{2}} = \sum_{i=0}^{\sfrac{n}{2}} \binom{n}{i}\cdot p_f^i \cdot (1 - p_f)^{n - i}
\end{equation}

\begin{table}
	\begin{center}
            \begin{tabular}{c|*{4}{c}}
                samples & 1 round & 2 rounds & 4 rounds & 10 rounds \\ \hline \rule{0pt}{3ex}
                1   & 0.7    &  0.91   &  0.9163 &  0.9527 \\
                10  & 0.8497 &  0.9774 &  0.9880 &  0.9986 \\
                25  & 0.9022 &  0.9904 &  0.9965 &  0.9999  \\
                50  & 0.9522 &  0.9977 &  0.9996 &  0.9999 \\
                100 & 0.9875 &  0.9998 &  0.9999 &  1.
            \end{tabular}
	\end{center}
    \caption{Probabilities that at most half of the rounds fail for different number of samples and different numbers of repetitions.}
    \label{tab:repeated_test_probs}
\end{table}

From the estimations \tabref{tab:repeated_test_probs} we infer that it is sufficient to test the high sampled models way less then the low sampled ones. Each test if not noted different will be the mean results of $\{20, 20, 8, 4, 2\}$ rounds for $\{1, 10, 25, 50, 100\}$ samples, respectively. Through this cascade scheme we reduced the needed test rounds from 100 to only 54 without losing accuracy in test results.

\clearpage
\section{Baseline}
\label{sec:results:baseline}
To compare out results we additionally solve the task with a baseline method. We build a simple object detector inspired by \citet{felzenszwalb_object_2010} using \acrfull{hog} \citep{dalal_histograms_2005}.\\

\subsection{\Gls{hog}}
\begin{wrapfigure}{r}{0.45\textwidth}
  \vspace{-25pt}
  \begin{center}
    \includegraphics[width=0.17\textwidth]{figures/hog_image.png}
    \includegraphics[width=0.17\textwidth]{figures/hog_feature.png}
  \end{center}
  \vspace{-5pt}
  \caption{A example of a patch and  the resulting \gls{hog} cells}
  \vspace{-10pt}
  \label{fig:hog}
\end{wrapfigure}
\gls{hog} features encode the shape of an object by assuming that its shape can be described by the distribution of edge directions. As the name already says we want to look at the gradients in the image at different orientations. We compute the horizontal and vertical gradients from the relative luminance of the original image using the standard weighted channel contributions. To get the spatial localization we split the image into small regions called cells. For each cell we compute the histogram which is just an 1-D vector containing the gradients over a fixed set of angles in that cell. Then we group these cells into larger overlapping blocks. The blocks are used to normalize the contained histogram vectors. This introduces better invariance to illumination and edge contrast. As the blocks are overlapping each cell will appear multiple times in the output but under different normalization. For normalization we use the L2-norm followed by clipping \citep{lowe_distinctive_2004}.

\subsection{Training}
We collect $1000$ samples of augmented samples using the same method as described in \treft{sec:pipeline:training:augment}. For all samples we compute the \gls{hog} features using the settings as in \citep{dalal_histograms_2005}. In \citet{felzenszwalb_object_2010} their model consists of multiple parts each described by its own \gls{hog} features. Because we will only train for small parts we describe each sample by only one feature vector. Using the set of balanced positive and negative samples we train a linear \gls{svm}.
\clearpage
\subsection{Testing}
The testing pipeline should be as similar as possible to the novel method. To use the same box generation, density calculation and \gls{nms} as in \treft{sec:pipeline:eval} we need a map of class probability predictions as output. Sliding a window over the images and computing the \gls{hog} features to classify them is rather inefficient. Instead we compute the feature map for the whole image and classify a window that is slided over the feature blocks. The \gls{svm} returns the predicted class (background \textit{vs} foreground) and the scores are aggregated for all windows to build the needed probability map. Although it is not useful to train a multi-scale classifier for our small training samples we have to take differently scaled test objects into account. Thus we test on multiple scales. Because the \gls{svm} needs the same number of \gls{hog} blocks to predict their class we change the \gls{hog} parameters for the different scales. For windows half the size we also halve the width of each \gls{hog} cell thus resulting in the same number of blocks per window.

\clearpage
\section{Detection results}
\label{sec:results:results}
\begin{figure}[htb]
    \setlength\tabcolsep{0pt}
    \renewcommand{\arraystretch}{0}
    \begin{tabular}{ccccccc}
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_2008_002067} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_2009_004323} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_2009_004784} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_2009_005222} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_2009_002715} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_2010_005967} \\
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_bbox_2008_002067} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_bbox_2009_004323} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_bbox_2009_004784} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_bbox_2009_005222} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_bbox_2009_002715} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_bbox_2010_005967} \\[3pt]

      \includegraphics[height=1.8cm]{figures/hm_examples/50s_2008_002067} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_2009_004323} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_2009_004784} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_2009_005222} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_2009_002715} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_2010_005967} \\
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_bbox_2008_002067} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_bbox_2009_004323} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_bbox_2009_004784} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_bbox_2009_005222} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_bbox_2009_002715} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_bbox_2010_005967}

    \end{tabular}
	\caption{Examples from \code{person\_hair}: Top detections are from a run with 1 sample, bottom detections are from a run with 50 samples. Note how the 50 sampled network detects more variations of the parts while the other one does not. Also note how both networks miss-detect the cats head as human hair.}
  \label{fig:hm_examples}
\end{figure}
We test the pipeline for the class-parts combinations $(c,p)$ listed in appendix \ref{sec:appendix:combos} using the cascade scheme described in \treft{sec:results:repeat}. To evaluate the detection we collect all predicted bounding boxes for all images

For a summary of test results see \figreft{fig:auc_heatmap}. We clearly see the correlation between set size and the success in testing. Training with only one sample returns the worst results by distance with a mean \gls{auc} over all parts and all runs of 0.72. With increasing set size the test results saturate quite quickly. The mean \gls{auc} for all other set sizes lie inside a window of only 0.03. That may be explainable with our training settings which are designed for fast training and has no time to incorporate a high number of examples into the representation learning.

Besides the anticipated correlation between set size and test success we also show the correlation between different object parts and the success in quickly learning their representation. An example for this difference is given in \figreft{fig:pr_example}. Training on human hair was one of the more successful experiments while training on human necks does not return a useful classifier. The difference can be explained by the amount of discriminating structure a part has to offer. While necks most of the time are very small mostly contrast-less and structure-less patches hair has a distinguishable surface and thus more recognizable features.

\begin{figure}
  \begin{tabular}{cc}
    \includegraphics[width=0.47\textwidth]{figures/build/person_hair_precs_recs.pdf} &
    \includegraphics[width=0.47\textwidth]{figures/build/person_neck_precs_recs.pdf}
  \end{tabular}
  \caption{Precision-Recall curves for two parts. \code{person\_hair} on the left as an successful example and \code{person\_neck} as an example for utterly failed training. Certainly though we see in both plots the clear separation between different set sizes.}
  \label{fig:pr_example}
\end{figure}

Another potential explanation lies in our training method. We train our classifier not on the pixel-precise masks that \textsc{Pascal}-Part provide but on rectangular patches retrieved from the original images around each part. As such a patch of human hair will contain a section of the head underneath. One therefore can expect that a network not only learns the presentation of the part itself but also of the surrounding object segments. We see this relation in the quite similar results of \code{person\_hair} and \code{person\_head}. Firstly such spatial relations are only relevant in some cases and secondly while this may frustate part-based training in some cases most of the time spatial relations to semantically related parts are an actual indicator for the searched part. If we search for human hair and find human heads then we also found many instances of human hair.

Furthermore we want to check that the cascade testing scheme was long enough to report sufficiently precise results. We calculate the variance of \gls{auc} scores for the different parts and training sizes to verify their precision \fref{fig:auc_var_heatmap}. As we expected the variance drastically decreases with increasing set size and as such we conclude our testing scheme to be adequate for this random sampling. Even for the 20 times repeated 1 sampled training runs the variance of \gls{auc} scores remains under 0.02 most of the times.
\begin{figure}[p!h]
    \centering
    \input{figures/auc_heatmap.pgf}
	\caption{\gls{auc} of the \gls{roc} curves at different classes and training set sizes}
    \label{fig:auc_heatmap}
\end{figure}
\begin{figure}[p!h]
    \centering
    \input{figures/auc_var_heatmap.pgf}
	\caption{Variance of the \gls{auc} at different classes and training set sizes}
    \label{fig:auc_var_heatmap}
\end{figure}

\section{Time performance evaluation}
\label{sec:results:time}
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/timings_fig}
  \caption{Distribution of durations in testing and training. Time in training measured in seconds for training over 500 iterations. Time in testing is measured in ms per image completing forwarding and all postprocessing.}
  \label{fig:timings}
\end{figure}
For the application of this method in a live production setting speed is from huge importance. Therefore we measure and report the duration of training and deployment of the network \fref{fig:timings}. Training takes 3.5 minutes on a single Titan X and testing runs at around 55ms per image. With therefore around 18 images / second we are reaching real-time search in image databases.\\
Although this is a short time for a train in test run of a neural network it  nevertheless does reduce the applicability of our approach. If we assume a art image database of 10.000 images a query and complete search through the database would take $t \approx \SI{210}{\second} + \sfrac{10000}{18}s = \SI{12.8}{\minute}$.

There might be options to improve the time performance. One idea might be to precompute the last feature maps for all images so that while forwarding we only have to compute a single convolution and the bounding box retrieval. This might drastically improve retrieval speed but training the network still causes a practical barrier.
