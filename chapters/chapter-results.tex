% !TEX root = ../thesis.tex
%
\chapter{Test results}
\label{sec:results}
Testing of the method is done on the \textsc{Pascal}-Part dataset \citep{Chen_2014_CVPR} which provides part-based segmentation annotations for the \textsc{Pascal}-VOC 2010 challenge dataset \citep{pascal-voc-2010}. For the 20 object classes the dataset contains binary segmentation maps for parts of these objects.\\
For testing the detection method we train a model with different number of images. From \textsc{Pascal}-Part we pick sets of classes $c \subset C = \{\code{bird, person, train,}\dots\}$ and parts $p \subset P = \{\code{head, beak, lhand,}\dots\}$. We gather all images from \textsc{Pascal}-VOC 2010 which contain parts $p$ from the classes $c$. Then we extract all patches of these images around the selected parts. We call this set of image patches $I_{c,p}$, e.g. all images of beaks of birds. Then we pick a random subset of $k$ images from $I_{c,p}$ and train the network with this subset. We want to investigate the influence of $k$ on detection results.\\

\section{Repeated training and testing}
\label{sec:results:repeat}
Obviously we also have to consider the quality of the subset not only its size. As we know that a part of the generated and augmented samples is way to small or contains no significant structure to learn we have to assume that some image patches are not useful for training. Some patches are smaller than $10\times10$ pixels and others are nearly monochrome. To avoid the influence of bad images we repeat the training with one set size $k$ multiple times and calculate the mean results of the multiple runs.

Lets assume 30\% of all image patches are useless for training. For how many times do we have to repeat the training with $k$ images so that we can neglect the influence of these bad samples? To get a rough estimate we reduce this problem to a binary one. First we compute how probable it is that a training will fail for one $k$.\\
The probability $p_b$ that one image is bad is $0.3$. Lets further assume for simplicity that a training will still succeed sufficiently if up to $v = 40\%$ of the samples are bad. We get the probability that the training will succeed from the \gls{cdf} of the binomial distribution:
\begin{equation}
    p_s(k) = B_{k,p_b, v k} = \sum_{i=0}^{v k} \binom{k}{i}\cdot p_b^i \cdot (1 - p_b)^{k - i}
\end{equation}
For $k\in\{1, 10, 25, 50, 100\}$ we get $p_s = \{\num{0.7},\num{0.85},\num{0.902},\num{0.952},\num{0.988}\}$, respectively. These are the probabilities that we will not fail one specific training. The probability that a training fails is $p_f = 1 - p_s = \{\num{0.3},\num{0.15},\num{0.098},\num{0.048},\num{0.012}\}$. Now we want to look at the probability for an $n$ times repeated run of $k$ images that half or less of these rounds fail. Again this can be computed with the \gls{cdf}:
\begin{equation}
    p_{repeat}(n,p_f) = B_{n,p_f, \sfrac{n}{2}} = \sum_{i=0}^{\sfrac{n}{2}} \binom{n}{i}\cdot p_f^i \cdot (1 - p_f)^{n - i}
\end{equation}

\begin{table}
	\begin{center}
            \begin{tabular}{c|*{4}{c}}
                samples & 1 round & 2 rounds & 4 rounds & 10 rounds \\ \hline \rule{0pt}{3ex}
                1   & 0.7    &  0.91   &  0.9163 &  0.9527 \\
                10  & 0.8497 &  0.9774 &  0.9880 &  0.9986 \\
                25  & 0.9022 &  0.9904 &  0.9965 &  0.9999  \\
                50  & 0.9522 &  0.9977 &  0.9996 &  0.9999 \\
                100 & 0.9875 &  0.9998 &  0.9999 &  1.
            \end{tabular}
	\end{center}
    \caption{Probabilities that at most half of the rounds fail for different number of samples and different numbers of repetitions.}
    \label{tab:repeated_test_probs}
\end{table}

From the estimations \tabref{tab:repeated_test_probs} we infer that it is sufficient to test the high sampled models way less then the low sampled ones. Each test if not noted different will be the mean results of $\{20, 20, 8, 4, 2\}$ rounds for $\{1, 10, 25, 50, 100\}$ samples, respectively. Through this pyramid scheme we reduced the needed test rounds from 100 to only 54 without losing accuracy in test results.


\section{Baseline}
\label{sec:results:baseline}
To compare out results we additionally solve the task with a baseline method. We build a simple object detector inspired by \citet{felzenszwalb_object_2010} using \gls{hog} features \citep{dalal_histograms_2005}.\\

\subsection{\Gls{hog}}
\begin{wrapfigure}{r}{0.45\textwidth}
  \vspace{-25pt}
  \begin{center}
    \includegraphics[width=0.17\textwidth]{figures/hog_image.png}
    \includegraphics[width=0.17\textwidth]{figures/hog_feature.png}
  \end{center}
  \vspace{-5pt}
  \caption{A example of a patch and  the resulting \gls{hog} cells}
  \vspace{-10pt}
  \label{fig:hog}
\end{wrapfigure}
\gls{hog} features encode the shape of an object by assuming that its shape can be described by the distribution of edge directions. As the name already says we want to look at the gradients in the image at different orientations. We compute the horizontal and vertical gradients from the relative luminance of the original image using the standard weighted channel contributions. To get the spatial localization we split the image into small regions called cells. For each cell we compute the histogram which is just an 1-D vector containing the gradients over a fixed set of angles in that cell. Then we group these cells into larger overlapping blocks. The blocks are used to normalize the contained histogram vectors. This introduces better invariance to illumination and edge contrast. As the blocks are overlapping each cell will appear multiple times in the output but under different normalization. For normalization we use the L2-norm followed by clipping \citep{lowe_distinctive_2004}.

\subsection{Training}
We collect $1000$ samples of augmented samples using the same method as described in \treft{sec:pipeline:training:augment}. For all samples we compute the \gls{hog} features using the settings as in \citep{dalal_histograms_2005}. In \citet{felzenszwalb_object_2010} their model consists of multiple parts each described by its own \gls{hog} features. Because we will only train for small parts we describe each sample by only one feature vector. Using the set of balanced positive and negative samples we train a linear \gls{svm}.

\subsection{Testing}
The testing pipeline should be as similar as possible to the novel method. To use the same box generation, density calculation and \gls{nms} as in \treft{sec:pipeline:eval} we need a map of class probability predictions as output. Sliding a window over the images and computing the \gls{hog} features to classify them is rather inefficient. Instead we compute the feature map for the whole image and classify a window that is slided over the feature blocks. The \gls{svm} returns the predicted class (background \textit{vs} foreground) and the scores are aggregated for all windows to build the needed probability map. Although it is not useful to train a multi-scale classifier for our small training samples we have to take differently scaled test objects into account. Thus we test on multiple scales. Because the \gls{svm} needs the same number of \gls{hog} blocks to predict their class we change the \gls{hog} parameters for the different scales. For windows half the size we also halve the width of each \gls{hog} cell thus resulting in the same number of blocks per window.


\section{Detection results}
\label{sec:results:results}
\begin{figure}[htb]
    \setlength\tabcolsep{0pt}
    \renewcommand{\arraystretch}{0}
    \begin{tabular}{ccccccc}
      \includegraphics[height=1.8cm]{figures/person_hair/1sample/2008_002067} &
      \includegraphics[height=1.8cm]{figures/person_hair/1sample/2009_004323} &
      \includegraphics[height=1.8cm]{figures/person_hair/1sample/2009_004784} &
      \includegraphics[height=1.8cm]{figures/person_hair/1sample/2009_005222} &
      \includegraphics[height=1.8cm]{figures/person_hair/1sample/2009_002715} &
      \includegraphics[height=1.8cm]{figures/person_hair/1sample/2010_005967} \\

      \includegraphics[height=1.8cm]{figures/person_hair/50sample/2008_002067} &
      \includegraphics[height=1.8cm]{figures/person_hair/50sample/2009_004323} &
      \includegraphics[height=1.8cm]{figures/person_hair/50sample/2009_004784} &
      \includegraphics[height=1.8cm]{figures/person_hair/50sample/2009_005222} &
      \includegraphics[height=1.8cm]{figures/person_hair/50sample/2009_002715} &
      \includegraphics[height=1.8cm]{figures/person_hair/50sample/2010_005967} \\
    \end{tabular}
	\caption{Top detections are from a run with 1 sample, bottom detections are from a run with 50 samples}
    \label{fig:hm_examples}
\end{figure}
\begin{figure}[p!]
    \centering
    \input{figures/build/auc_heatmap.pgf}
	\caption{\gls{auc} of the \gls{roc} curves at different classes and training set sizes}
    \label{fig:auc_heatmap}
\end{figure}
\begin{figure}[p!]
    \centering
    \input{figures/build/auc_var_heatmap.pgf}
	\caption{Variance of the \gls{auc} at different classes and training set sizes}
    \label{fig:auc_heatmap}
\end{figure}

\section{Time performance evaluation}
\label{sec:results:time}
