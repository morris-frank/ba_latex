% !TEX root = ../thesis.tex
%
\chapter{Test results}
\label{sec:results}
Testing of the method is done on the \textsc{Pascal}-Part dataset \citep{Chen_2014_CVPR}, which provides part-based segmentation annotations for images of the \textsc{Pascal}-VOC 2010 challenge dataset \citep{pascal-voc-2010}. For the 20 object classes \textsc{Pascal}-Part contains binary segmentation maps for a selected number of parts of all objects classes.\\
For testing the detection method we train a model with different number of images. From \textsc{Pascal}-Part we pick sets of classes $c \subset C = \{\code{bird, person, train,}\dotsc\}$ and parts $p \subset P = \{\code{head, beak, lhand,}\dotsc\}$. We gather all images from \textsc{Pascal}-VOC 2010 which contain parts $p$ from the classes $c$. Then we extract all patches of these images around the selected parts. We call this set of image patches $I_{c,p}$, e.g. all images of beaks of birds. Then we pick a random subset $I_k$ of $k$ images from $I_{c,p}$ and train the network with this subset. We want to investigate the influence of $k$ on detection results. The remaining set of images $I_{c,p} \setminus I_k$ will be the test set for this single network. For each set $(c,p)$ we take random patches out of the corresponding \textsc{Pascal}-VOC images guaranteeing that they do not overlap with images in $I_{c,p}$. These patches are our negative samples for $(c,p)$ and will be fed into the same data augmentation as the positive samples.

Following in the next chapter we first rationalize our reduced and unequal number of tests \tref{sec:results:repeat}. Next in \treft{sec:results:baseline} we quickly explain the baseline method, which was used as comparison for our results. Finally in \treft{sec:results:results} we report our test results in detection success and time performance \tref{sec:results:time}.

\section{Repeated training and testing}
\label{sec:results:repeat}
Obviously we also have to consider the quality of the subset, not only its size. As we know that a part of the generated and augmented samples is way to small or contains no significant structure to learn, we have to assume that some image patches are not useful for training. Some patches are smaller than $10\times10$ pixels, and others are nearly monochrome. To avoid the influence of bad images, we repeat the training with one set size $k$ multiple times and calculate the mean results of the multiple runs.

Lets assume 30\% of all image patches are useless for training. For how many times do we have to repeat the training with $k$ images, so that we can neglect the influence of these bad samples? To get a rough estimate we reduce this problem to a binary one. First we compute how probable it is that a training will fail for one $k$.\\
The probability $p_b$ that one image is bad is $0.3$. Lets further assume, for simplicity, that a training will still sufficiently succeed if up to $v = 40\%$ of the samples are bad. We get the probability that the training will succeed from the \gls{cdf} of the binomial distribution:
\begin{equation}
    p_s(k) = B_{k,p_b, v k} = \sum_{i=0}^{v k} \binom{k}{i}\cdot p_b^i \cdot (1 - p_b)^{k - i}
\end{equation}
For $k\in\{1, 10, 25, 50, 100\}$ this returns $p_s = \{\num{0.7},\num{0.85},\num{0.902},\num{0.952},\num{0.988}\}$, respectively. These are the probabilities that we will not fail one specific training. The probability that a training fails is $p_f = 1 - p_s = \{\num{0.3},\num{0.15},\num{0.098},\num{0.048},\num{0.012}\}$. Now we want to look at the probability, for an $n$ times repeated run of $k$ images, that half or less of these rounds fail. Again this can be computed with the \gls{cdf}:
\begin{equation}
    p_{repeat}(n,p_f) = B_{n,p_f, \sfrac{n}{2}} = \sum_{i=0}^{\sfrac{n}{2}} \binom{n}{i}\cdot p_f^i \cdot (1 - p_f)^{n - i}
\end{equation}

\begin{table}
	\begin{center}
            \begin{tabular}{c|*{4}{c}}
                samples & 1 round & 2 rounds & 4 rounds & 10 rounds \\ \hline \rule{0pt}{3ex}
                1   & 0.7    &  0.91   &  0.9163 &  0.9527 \\
                10  & 0.8497 &  0.9774 &  0.9880 &  0.9986 \\
                25  & 0.9022 &  0.9904 &  0.9965 &  0.9999  \\
                50  & 0.9522 &  0.9977 &  0.9996 &  0.9999 \\
                100 & 0.9875 &  0.9998 &  0.9999 &  1.
            \end{tabular}
	\end{center}
    \caption{Probabilities that at most half of the rounds fail for different number of samples and different numbers of repetitions.}
    \label{tab:repeated_test_probs}
\end{table}

From the estimations \tabref{tab:repeated_test_probs} we infer that it is sufficient to test the high sampled models way less, than the low sampled ones. Each test, if not noted different, will be the mean results of $\{20, 20, 8, 4, 2\}$ rounds for $\{1, 10, 25, 50, 100\}$ samples, respectively. Through this cascade scheme we reduced the needed test rounds from 100 to only 54, without losing accuracy in test results.

\clearpage
\section{Baseline}
\label{sec:results:baseline}
To compare our results we additionally solve the task with a baseline method. We build a simple object detector inspired by \citet{felzenszwalb_object_2010} using \acrfull{hog} \citep{dalal_histograms_2005}.\\

\subsection{\Gls{hog}}
\begin{wrapfigure}{r}{0.45\textwidth}
  \vspace{-25pt}
  \begin{center}
    \includegraphics[width=0.17\textwidth]{figures/hog_image.png}
    \includegraphics[width=0.17\textwidth]{figures/hog_feature.png}
  \end{center}
  \vspace{-5pt}
  \caption{A example of a patch and  the resulting \gls{hog} cells.}
  \vspace{-10pt}
  \label{fig:hog}
\end{wrapfigure}
\gls{hog} features encode the shape of an object assuming that its shape can be described by the distribution of edge directions. As the name already states we want to look at the gradients of the image at different orientations. We compute the horizontal and vertical gradients from the relative luminance of the original image, using the standard weighted channel contributions. To get the spatial localization we split the image into small regions called cells. For each cell we compute the histogram, which is just an $1d$ vector containing the gradients over a fixed set of angles in that cell. Next we group these cells into larger overlapping blocks. The blocks are used to normalize the contained histograms. This introduces better invariance to illumination and edge contrast. As the blocks are overlapping, each cell will appear multiple times in the output but under different normalization. For normalization we use the L2-norm followed by clipping \citep{lowe_distinctive_2004}.

\subsection{Training}
We collect $1000$ samples of augmented samples, using the same method as described in \treft{sec:pipeline:training:augment}. For all samples we compute the \gls{hog} features using the settings as in \citep{dalal_histograms_2005}. In \citet{felzenszwalb_object_2010} their model consists of multiple parts each described by its own \gls{hog} features. Because we will only train for small parts we describe each sample by only one \gls{hog} descriptor at one scale. Using the set of balanced positive and negative samples we train a linear \gls{svm}.
\clearpage
\subsection{Testing}
The testing pipeline should be as similar as possible to the novel method. To use the same box generation, density calculation and \gls{nms} as in \treft{sec:pipeline:eval}, we need a map of class probability predictions as input. Sliding a window over the images and computing the \gls{hog} features for all positions in the image is rather inefficient. Instead we compute the \gls{hog} descriptors for the whole image and classify a window that is slided over the feature map. The \gls{svm} returns the predicted class (background \textit{vs} foreground), and the scores are aggregated for all windows to build the needed probability map. Although it is not useful to train a multi-scale classifier for our small training samples, we have to take differently scaled test objects into account. Thus we test on multiple scales. Because the \gls{svm} needs a fixed shape, the same number of \gls{hog} blocks, as input to predict their class, we change the \gls{hog} parameters for the different scales. For windows half the size we also halve the width of each \gls{hog} cell, thus resulting in the same number of blocks per window.

\clearpage
\section{Detection results}
\label{sec:results:results}
\begin{figure}[htb]
    \setlength\tabcolsep{0pt}
    \renewcommand{\arraystretch}{0}
    \begin{tabular}{ccccccc}
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_2008_002067} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_2009_004323} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_2009_004784} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_2009_005222} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_2009_002715} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_2010_005967} \\
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_bbox_2008_002067} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_bbox_2009_004323} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_bbox_2009_004784} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_bbox_2009_005222} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_bbox_2009_002715} &
      \includegraphics[height=1.8cm]{figures/hm_examples/1s_bbox_2010_005967} \\[3pt]

      \includegraphics[height=1.8cm]{figures/hm_examples/50s_2008_002067} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_2009_004323} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_2009_004784} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_2009_005222} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_2009_002715} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_2010_005967} \\
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_bbox_2008_002067} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_bbox_2009_004323} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_bbox_2009_004784} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_bbox_2009_005222} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_bbox_2009_002715} &
      \includegraphics[height=1.8cm]{figures/hm_examples/50s_bbox_2010_005967}

    \end{tabular}
	\caption{Examples from \code{person\_hair}: Top detections are from a run with 1 sample, bottom detections are from a run with 50 samples. Note how the 50 sampled network detects more variations of the parts while the other one does not. Also note how both networks miss-detect the cats head as human hair.}
  \label{fig:hm_examples}
\end{figure}
We test the pipeline for the class-parts combinations $(c,p)$ listed in appendix \ref{sec:appendix:combos}, using the cascade scheme described in \treft{sec:results:repeat}. To evaluate the detection we collect all predicted bounding boxes for all images and their scores.

For a summary of test results see \figreft{fig:auc_heatmap}. We clearly see the correlation between set size and the success in testing. Training with only one sample returns the worst results, by distance, with a mean \gls{auc} over all parts and all runs of 0.72. With increasing set size the test results saturate quickly. The mean \gls{auc} for all larger set sizes lie inside a range of only 0.03. That may be explainable with our training settings, which are designed for fast training and has no time to incorporate a high number of examples into the representation learning.\TODO{add baseline comparsion}

Besides the anticipated correlation between set size and test success, we also show the correlation between different object parts and the success in quickly learning their representation. An example for this difference is given in \figreft{fig:pr_example}. Training on human hair was one of the more successful experiments while training on human necks does not return a useful classifier. The difference can be explained by the amount of discriminating structure a part has to offer. While necks most of the time are very small mostly contrast-less and structure-less patches, hair has a distinguishable surface and therefore more recognizable visual features.

\begin{figure}
  \begin{tabular}{cc}
    \prplot{person_hair} &
    \prplot{person_neck}
  \end{tabular}
  \caption{Precision-Recall curves for two parts. \code{person\_hair} on the left as an successful example and \code{person\_neck} as an example for an utterly failed training. Certainly though we see in both plots the clear separation between different set sizes.}
  \label{fig:pr_example}
\end{figure}

Another potential explanation stems from our training method. We train our classifier not on the pixel-precise masks, that \textsc{Pascal}-Part provide, but on rectangular patches retrieved from the original images around each part. As such a patch of human hair will, most of the time, contain a section of the head underneath. One therefore can expect that the model not only learns the representation of the part itself but also of the surrounding object segments. We see this relation in the quite similar results of \code{person\_hair} and \code{person\_head}. Firstly such spatial relations are only relevant in some cases, and secondly while this may frustrate part-based training in some cases, most of the time spatial relations to semantically related parts are an actual indicator for the searched part. If we search for human hair and find human heads, we also found many instances of human hair.

Furthermore we want to check that the cascade testing scheme was intensive enough to report sufficiently precise results. We calculate the variance of \gls{auc} scores for the different parts and training sizes to verify their precision \fref{fig:auc_var_heatmap}. As we expected the variance drastically decreases with increasing set size, and as such we conclude our testing scheme to be adequate for this random sampling. Even for the 20 times repeated 1 sampled training runs, the variance of \gls{auc} scores remains under 0.02 most of the times.
\begin{figure}[p!h]
    \centering
    \input{figures/auc_heatmap.pgf}
	\caption{\gls{auc} of the \gls{roc} curves at different classes and training set sizes.}
    \label{fig:auc_heatmap}
\end{figure}
\begin{figure}[p!h]
    \centering
    \input{figures/auc_var_heatmap.pgf}
	\caption{Variance of the \gls{auc} at different classes and training set sizes.}
    \label{fig:auc_var_heatmap}
\end{figure}

\section{Time performance evaluation}
\label{sec:results:time}
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/timings_fig}
  \caption{Distribution of durations in testing and training. Time in training measured in seconds for training over 500 iterations. Time in testing is measured in ms per image completing forwarding and all postprocessing.}
  \label{fig:timings}
\end{figure}
For the application of this method in a live production setting, speed is from huge importance. Therefore we measure and report the duration for training and deployment of the network \fref{fig:timings}. Training takes around 3.5 minutes on a single Titan X and testing runs at around 55ms per image. With therefore approximately 18 images / second we are reaching real-time search in image databases.\\
Although this is a short time for a train and test run of a neural network it  nevertheless does reduce the applicability of this approach. If we assume an art image database of 10.000 images, a query and complete search through the database would take $t \approx \SI{210}{\second} + \sfrac{10000}{18}s = \SI{12.8}{\minute}$.

There might be options to improve the time performance. One idea is to precompute the last feature maps for all images, so that, while searching through the images, we only have to compute a single convolution and the bounding box retrieval. This might drastically improve retrieval speed but training the network still causes a practical barrier.
